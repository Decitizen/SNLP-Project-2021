{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJ6MhJYYBCwu",
    "outputId": "aa6380f0-736d-463e-846d-f1835edfa7ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 28 15:08:36 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   44C    P0    30W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmj22-TcZMef"
   },
   "source": [
    "## Setup\n",
    "\n",
    "We'll need [the Transformers library](https://huggingface.co/transformers/) by Hugging Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kj_7Tz0-pK69"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jjsbi1u3QFEM"
   },
   "outputs": [],
   "source": [
    "!pip install -qq transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AJqoaFpVpoM8",
    "outputId": "58675754-4001-409c-8888-5c059bfb6a57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.7.10\n",
      "IPython version      : 5.5.0\n",
      "\n",
      "numpy       : 1.19.5\n",
      "pandas      : 1.1.5\n",
      "torch       : 1.8.1+cu101\n",
      "transformers: 4.5.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -v -p numpy,pandas,torch,transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w68CZpOwFoly",
    "outputId": "41accf41-94d3-472d-866b-b9072a1d51e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Setup & Config\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufzPdoTtNikq"
   },
   "source": [
    "## Data Exploration\n",
    "\n",
    "We'll load the Google Play app reviews dataset, that we've put together in the previous part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_1aIzoaWsGxJ",
    "outputId": "216d0b63-1414-48d2-c805-07ef2675481c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cL7HcOw0sO1i",
    "outputId": "aa3a22f7-d182-4f15-c53a-e5c34ccce93a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "community_labels.csv\n"
     ]
    }
   ],
   "source": [
    "ls \"/content/gdrive/My Drive/SNLP project/data/community_labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7NsCEgOmsyKy",
    "outputId": "6faef236-a943-408d-c17a-927848d79ee7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jsonlines in /usr/local/lib/python3.7/dist-packages (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install jsonlines\n",
    "!pip install -q -U watermark\n",
    "!pip install -qq transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dq0CUmJnscMt"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cviREoW0g2wN"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsC2UVdtHUvc"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1SsGPJfGzIGu"
   },
   "outputs": [],
   "source": [
    "PROJECT_PATH = \"/content/gdrive/MyDrive/SNLP project/\"\n",
    "DATA_FOLDER_PATH = 'data/preprocessed_tweets/'\n",
    "\n",
    "df_clean_sample = pd.read_csv(PROJECT_PATH + DATA_FOLDER_PATH + 'df_clean_sampled_binary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQxfP2ykSSMb"
   },
   "outputs": [],
   "source": [
    "LR = 2e-5 # Learning rate\n",
    "EPOCHS = 4\n",
    "DROPOUT = 0.3\n",
    "N_CLUSTERS = 2\n",
    "BATCH_SIZE = 32\n",
    "MAX_SEQ_LEN = 74\n",
    "WARMUP_STEPS = 0\n",
    "\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3eq7wKhZKF4"
   },
   "source": [
    "## Preprocess tweet content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GMuEH6IZR98"
   },
   "source": [
    "## Grid search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SuLCTP6hZljZ",
    "outputId": "960de11a-1b13-4526-a5fe-837ef828b6a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==0.24.1 in /usr/local/lib/python3.7/dist-packages (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.1) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.1) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.1) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.1) (2.1.0)\n",
      "Requirement already satisfied: skorch in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from skorch) (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from skorch) (1.19.5)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from skorch) (1.4.1)\n",
      "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.7/dist-packages (from skorch) (4.41.1)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from skorch) (0.8.9)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->skorch) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->skorch) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn==0.24.1\n",
    "!pip install skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCb4nt6jFFYA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self, n_classes, dropout):\n",
    "    super(SentimentClassifier, self).__init__()\n",
    "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "    self.drop = nn.Dropout(p=dropout)\n",
    "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "  \n",
    "  def forward(self, input_ids, attention_mask, **kwargs):\n",
    "    pooled_output = self.bert(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "    output = self.drop(pooled_output[1])\n",
    "    return self.out(output)\n",
    "\n",
    "class TwitterClimateSkorchDataset(Dataset):\n",
    "\n",
    "  def __init__(self, data, tokenizer, max_len):\n",
    "    self.tweets = data['content'].values\n",
    "    self.clusters = data['Cluster'].values\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.tweets)\n",
    "  \n",
    "  def __getitem__(self, item):\n",
    "    tweet = str(self.tweets[item])\n",
    "    cluster = self.clusters[item]\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      tweet,\n",
    "      #padding='max_length',\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids'].flatten().to(device)\n",
    "    attention_mask = encoding['attention_mask'].flatten().to(device)\n",
    "    clusters = torch.tensor(cluster, dtype=torch.long, device=device)\n",
    "\n",
    "    return input_ids, attention_mask, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EOmMVF-DKURe",
    "outputId": "64dea168-ac2f-492d-deb3-bad113524d40"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49598,)\n",
      "torch.Size([49598])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv # noqa\n",
    "from sklearn.model_selection import HalvingGridSearchCV, GridSearchCV\n",
    "#from skorch.classifier import NeuralNetClassifier\n",
    "from skorch.helper import SliceDict, SliceDataset\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# skorch & sklearn needs dataloader wrapped in SliceDataset for GridSearchCV\n",
    "ds = TwitterClimateSkorchDataset(df_clean_sample, tokenizer, MAX_SEQ_LEN)\n",
    "X0 = SliceDataset(ds, idx=0)\n",
    "X1 = SliceDataset(ds, idx=1)\n",
    "X_train = SliceDict(input_ids=X0, attention_mask=X1)\n",
    "y_train = SliceDataset(ds, idx=2)\n",
    "\n",
    "# y_train has to be fed into CPU memory to avoid errors?\n",
    "y_train = torch.tensor([y for y in iter(y_train)], dtype=torch.long, device=\"cpu\")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7iZwfIPjZ7WG",
    "outputId": "501c46f3-91f3-4040-ee71-925dd708b8c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 4\n",
      "n_required_iterations: 5\n",
      "n_possible_iterations: 4\n",
      "min_resources_: 1000\n",
      "max_resources_: 8000\n",
      "aggressive_elimination: False\n",
      "factor: 2\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 20\n",
      "n_resources: 1000\n",
      "Fitting 2 folds for each of 20 candidates, totalling 40 fits\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7100\u001b[0m  5.2408\n",
      "      2        \u001b[36m0.6617\u001b[0m  5.2323\n",
      "      3        \u001b[36m0.5844\u001b[0m  5.1951\n",
      "[CV] END batch_size=32, module__dropout=0.1, optimizer__lr=2e-05; total time=  21.3s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7136\u001b[0m  5.2414\n",
      "      2        \u001b[36m0.6618\u001b[0m  5.2449\n",
      "      3        \u001b[36m0.5463\u001b[0m  5.2392\n",
      "[CV] END batch_size=32, module__dropout=0.1, optimizer__lr=2e-05; total time=  21.3s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7101\u001b[0m  5.1995\n",
      "      2        \u001b[36m0.6943\u001b[0m  5.2087\n",
      "      3        \u001b[36m0.6412\u001b[0m  5.2112\n",
      "[CV] END batch_size=32, module__dropout=0.1, optimizer__lr=1e-05; total time=  21.3s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6979\u001b[0m  5.2505\n",
      "      2        \u001b[36m0.6741\u001b[0m  5.2641\n",
      "      3        \u001b[36m0.6302\u001b[0m  5.2397\n",
      "[CV] END batch_size=32, module__dropout=0.1, optimizer__lr=1e-05; total time=  21.3s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6894\u001b[0m  5.2233\n",
      "      2        \u001b[36m0.6367\u001b[0m  5.2298\n",
      "      3        \u001b[36m0.5661\u001b[0m  5.1909\n",
      "[CV] END batch_size=32, module__dropout=0.2, optimizer__lr=2e-05; total time=  21.3s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6976\u001b[0m  5.2517\n",
      "      2        \u001b[36m0.6842\u001b[0m  5.2127\n",
      "      3        \u001b[36m0.6522\u001b[0m  5.2203\n",
      "[CV] END batch_size=32, module__dropout=0.2, optimizer__lr=2e-05; total time=  21.5s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7036\u001b[0m  5.1885\n",
      "      2        \u001b[36m0.6877\u001b[0m  5.2091\n",
      "      3        \u001b[36m0.6509\u001b[0m  5.2029\n",
      "[CV] END batch_size=32, module__dropout=0.2, optimizer__lr=1e-05; total time=  21.2s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7320\u001b[0m  5.2394\n",
      "      2        \u001b[36m0.7060\u001b[0m  5.2457\n",
      "      3        \u001b[36m0.6905\u001b[0m  5.2284\n",
      "[CV] END batch_size=32, module__dropout=0.2, optimizer__lr=1e-05; total time=  21.3s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7050\u001b[0m  5.1802\n",
      "      2        \u001b[36m0.6660\u001b[0m  5.2011\n",
      "      3        \u001b[36m0.5824\u001b[0m  5.1968\n",
      "[CV] END batch_size=32, module__dropout=0.3, optimizer__lr=2e-05; total time=  21.2s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7035\u001b[0m  5.2594\n",
      "      2        \u001b[36m0.6956\u001b[0m  5.2281\n",
      "      3        \u001b[36m0.6352\u001b[0m  5.2313\n",
      "[CV] END batch_size=32, module__dropout=0.3, optimizer__lr=2e-05; total time=  21.3s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7162\u001b[0m  5.2408\n",
      "      2        \u001b[36m0.6872\u001b[0m  5.2027\n",
      "      3        \u001b[36m0.6221\u001b[0m  5.2178\n",
      "[CV] END batch_size=32, module__dropout=0.3, optimizer__lr=1e-05; total time=  21.3s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7513\u001b[0m  5.2672\n",
      "      2        \u001b[36m0.6673\u001b[0m  5.2206\n",
      "      3        \u001b[36m0.6149\u001b[0m  5.2779\n",
      "[CV] END batch_size=32, module__dropout=0.3, optimizer__lr=1e-05; total time=  21.4s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7409\u001b[0m  5.2153\n",
      "      2        \u001b[36m0.7169\u001b[0m  5.1966\n",
      "      3        \u001b[36m0.6397\u001b[0m  5.2230\n",
      "[CV] END batch_size=32, module__dropout=0.4, optimizer__lr=2e-05; total time=  21.3s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7374\u001b[0m  5.2806\n",
      "      2        \u001b[36m0.7076\u001b[0m  5.2145\n",
      "      3        \u001b[36m0.6845\u001b[0m  5.2255\n",
      "[CV] END batch_size=32, module__dropout=0.4, optimizer__lr=2e-05; total time=  21.4s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7683\u001b[0m  5.2358\n",
      "      2        \u001b[36m0.7157\u001b[0m  5.2042\n",
      "      3        \u001b[36m0.6744\u001b[0m  5.2314\n",
      "[CV] END batch_size=32, module__dropout=0.4, optimizer__lr=1e-05; total time=  21.4s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7418\u001b[0m  5.2458\n",
      "      2        \u001b[36m0.7234\u001b[0m  5.2546\n",
      "      3        \u001b[36m0.7043\u001b[0m  5.2583\n",
      "[CV] END batch_size=32, module__dropout=0.4, optimizer__lr=1e-05; total time=  21.4s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7223\u001b[0m  5.2698\n",
      "      2        \u001b[36m0.6665\u001b[0m  5.2265\n",
      "      3        \u001b[36m0.5840\u001b[0m  5.1859\n",
      "[CV] END batch_size=32, module__dropout=0.5, optimizer__lr=2e-05; total time=  21.3s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7311\u001b[0m  5.2953\n",
      "      2        \u001b[36m0.6691\u001b[0m  5.2348\n",
      "      3        \u001b[36m0.5646\u001b[0m  5.2298\n",
      "[CV] END batch_size=32, module__dropout=0.5, optimizer__lr=2e-05; total time=  21.3s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7619\u001b[0m  5.1662\n",
      "      2        \u001b[36m0.7373\u001b[0m  5.2327\n",
      "      3        \u001b[36m0.7211\u001b[0m  5.1862\n",
      "[CV] END batch_size=32, module__dropout=0.5, optimizer__lr=1e-05; total time=  21.1s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7388\u001b[0m  5.2756\n",
      "      2        \u001b[36m0.6866\u001b[0m  5.2139\n",
      "      3        \u001b[36m0.6260\u001b[0m  5.2555\n",
      "[CV] END batch_size=32, module__dropout=0.5, optimizer__lr=1e-05; total time=  21.3s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6973\u001b[0m  4.2524\n",
      "      2        \u001b[36m0.6442\u001b[0m  4.2590\n",
      "      3        \u001b[36m0.5344\u001b[0m  4.2547\n",
      "[CV] END batch_size=64, module__dropout=0.1, optimizer__lr=2e-05; total time=  18.1s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7103\u001b[0m  4.3204\n",
      "      2        \u001b[36m0.6632\u001b[0m  4.2883\n",
      "      3        \u001b[36m0.6089\u001b[0m  4.2809\n",
      "[CV] END batch_size=64, module__dropout=0.1, optimizer__lr=2e-05; total time=  19.3s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6978\u001b[0m  4.2447\n",
      "      2        \u001b[36m0.6676\u001b[0m  4.2682\n",
      "      3        \u001b[36m0.6474\u001b[0m  4.2554\n",
      "[CV] END batch_size=64, module__dropout=0.1, optimizer__lr=1e-05; total time=  18.0s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7079\u001b[0m  4.3138\n",
      "      2        \u001b[36m0.6848\u001b[0m  4.2855\n",
      "      3        \u001b[36m0.6828\u001b[0m  4.2793\n",
      "[CV] END batch_size=64, module__dropout=0.1, optimizer__lr=1e-05; total time=  18.1s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7536\u001b[0m  4.2823\n",
      "      2        \u001b[36m0.6549\u001b[0m  4.2588\n",
      "      3        \u001b[36m0.5591\u001b[0m  4.2585\n",
      "[CV] END batch_size=64, module__dropout=0.2, optimizer__lr=2e-05; total time=  18.0s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7280\u001b[0m  4.3042\n",
      "      2        \u001b[36m0.6966\u001b[0m  4.3129\n",
      "      3        \u001b[36m0.6651\u001b[0m  4.2949\n",
      "[CV] END batch_size=64, module__dropout=0.2, optimizer__lr=2e-05; total time=  18.1s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7023\u001b[0m  4.2488\n",
      "      2        \u001b[36m0.6700\u001b[0m  4.2534\n",
      "      3        \u001b[36m0.6158\u001b[0m  4.2440\n",
      "[CV] END batch_size=64, module__dropout=0.2, optimizer__lr=1e-05; total time=  18.0s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7016\u001b[0m  4.3255\n",
      "      2        \u001b[36m0.6780\u001b[0m  4.3040\n",
      "      3        \u001b[36m0.6574\u001b[0m  4.2869\n",
      "[CV] END batch_size=64, module__dropout=0.2, optimizer__lr=1e-05; total time=  18.2s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7168\u001b[0m  4.2569\n",
      "      2        \u001b[36m0.6889\u001b[0m  4.2484\n",
      "      3        \u001b[36m0.6562\u001b[0m  4.2469\n",
      "[CV] END batch_size=64, module__dropout=0.3, optimizer__lr=2e-05; total time=  18.0s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7922\u001b[0m  4.3070\n",
      "      2        \u001b[36m0.7146\u001b[0m  4.2952\n",
      "      3        \u001b[36m0.6405\u001b[0m  4.2772\n",
      "[CV] END batch_size=64, module__dropout=0.3, optimizer__lr=2e-05; total time=  18.1s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7221\u001b[0m  4.2526\n",
      "      2        \u001b[36m0.6870\u001b[0m  4.2346\n",
      "      3        \u001b[36m0.6674\u001b[0m  4.2489\n",
      "[CV] END batch_size=64, module__dropout=0.3, optimizer__lr=1e-05; total time=  17.9s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7070\u001b[0m  4.3111\n",
      "      2        \u001b[36m0.7055\u001b[0m  4.2955\n",
      "      3        \u001b[36m0.6796\u001b[0m  4.2952\n",
      "[CV] END batch_size=64, module__dropout=0.3, optimizer__lr=1e-05; total time=  18.1s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7663\u001b[0m  4.2361\n",
      "      2        \u001b[36m0.6945\u001b[0m  4.2452\n",
      "      3        \u001b[36m0.6645\u001b[0m  4.2594\n",
      "[CV] END batch_size=64, module__dropout=0.4, optimizer__lr=2e-05; total time=  18.0s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7427\u001b[0m  4.2868\n",
      "      2        \u001b[36m0.7282\u001b[0m  4.3016\n",
      "      3        \u001b[36m0.7031\u001b[0m  4.2839\n",
      "[CV] END batch_size=64, module__dropout=0.4, optimizer__lr=2e-05; total time=  18.1s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7429\u001b[0m  4.2560\n",
      "      2        \u001b[36m0.7115\u001b[0m  4.2500\n",
      "      3        \u001b[36m0.6987\u001b[0m  4.2490\n",
      "[CV] END batch_size=64, module__dropout=0.4, optimizer__lr=1e-05; total time=  18.2s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7286\u001b[0m  4.3298\n",
      "      2        \u001b[36m0.7178\u001b[0m  4.3144\n",
      "      3        \u001b[36m0.6925\u001b[0m  4.3102\n",
      "[CV] END batch_size=64, module__dropout=0.4, optimizer__lr=1e-05; total time=  18.2s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7399\u001b[0m  4.3165\n",
      "      2        \u001b[36m0.6691\u001b[0m  4.2544\n",
      "      3        \u001b[36m0.5691\u001b[0m  4.2639\n",
      "[CV] END batch_size=64, module__dropout=0.5, optimizer__lr=2e-05; total time=  18.1s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7715\u001b[0m  4.4293\n",
      "      2        \u001b[36m0.7105\u001b[0m  4.3163\n",
      "      3        \u001b[36m0.6329\u001b[0m  4.3442\n",
      "[CV] END batch_size=64, module__dropout=0.5, optimizer__lr=2e-05; total time=  18.4s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7768\u001b[0m  4.2850\n",
      "      2        \u001b[36m0.7372\u001b[0m  4.2929\n",
      "      3        \u001b[36m0.7323\u001b[0m  4.2534\n",
      "[CV] END batch_size=64, module__dropout=0.5, optimizer__lr=1e-05; total time=  18.1s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7528\u001b[0m  4.3304\n",
      "      2        \u001b[36m0.7401\u001b[0m  4.2897\n",
      "      3        \u001b[36m0.7136\u001b[0m  4.2964\n",
      "[CV] END batch_size=64, module__dropout=0.5, optimizer__lr=1e-05; total time=  18.1s\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 10\n",
      "n_resources: 2000\n",
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.6710\u001b[0m  10.4414\n",
      "      2        \u001b[36m0.5447\u001b[0m  10.4487\n",
      "      3        \u001b[36m0.3271\u001b[0m  10.4459\n",
      "[CV] END batch_size=32, module__dropout=0.2, optimizer__lr=2e-05; total time=  39.3s\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.6876\u001b[0m  10.5322\n",
      "      2        \u001b[36m0.5626\u001b[0m  10.4775\n",
      "      3        \u001b[36m0.3636\u001b[0m  10.4657\n",
      "[CV] END batch_size=32, module__dropout=0.2, optimizer__lr=2e-05; total time=  39.4s\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.6912\u001b[0m  10.4611\n",
      "      2        \u001b[36m0.5586\u001b[0m  10.4834\n",
      "      3        \u001b[36m0.3624\u001b[0m  10.4679\n",
      "[CV] END batch_size=32, module__dropout=0.3, optimizer__lr=2e-05; total time=  39.3s\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.7108\u001b[0m  10.5189\n",
      "      2        \u001b[36m0.6888\u001b[0m  10.4929\n",
      "      3        \u001b[36m0.5749\u001b[0m  10.5589\n",
      "[CV] END batch_size=32, module__dropout=0.3, optimizer__lr=2e-05; total time=  39.5s\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.6821\u001b[0m  10.5246\n",
      "      2        \u001b[36m0.6032\u001b[0m  10.5085\n",
      "      3        \u001b[36m0.4563\u001b[0m  10.5086\n",
      "[CV] END batch_size=32, module__dropout=0.1, optimizer__lr=1e-05; total time=  39.6s\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.6914\u001b[0m  10.5832\n",
      "      2        \u001b[36m0.6460\u001b[0m  10.5800\n",
      "      3        \u001b[36m0.5303\u001b[0m  10.5586\n",
      "[CV] END batch_size=32, module__dropout=0.1, optimizer__lr=1e-05; total time=  39.7s\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.7090\u001b[0m  10.5202\n",
      "      2        \u001b[36m0.6609\u001b[0m  10.4983\n",
      "      3        \u001b[36m0.4821\u001b[0m  10.4317\n",
      "[CV] END batch_size=32, module__dropout=0.4, optimizer__lr=2e-05; total time=  39.5s\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.7136\u001b[0m  10.5282\n",
      "      2        \u001b[36m0.6394\u001b[0m  10.5533\n",
      "      3        \u001b[36m0.4592\u001b[0m  10.5203\n",
      "[CV] END batch_size=32, module__dropout=0.4, optimizer__lr=2e-05; total time=  39.7s\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.7569\u001b[0m  11.5925\n",
      "      2        \u001b[36m0.7211\u001b[0m  8.5416\n",
      "      3        \u001b[36m0.6323\u001b[0m  8.5583\n",
      "[CV] END batch_size=64, module__dropout=0.5, optimizer__lr=2e-05; total time=  36.2s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7392\u001b[0m  8.6415\n",
      "      2        \u001b[36m0.6671\u001b[0m  8.5624\n",
      "      3        \u001b[36m0.5506\u001b[0m  8.5846\n",
      "[CV] END batch_size=64, module__dropout=0.5, optimizer__lr=2e-05; total time=  33.1s\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.6626\u001b[0m  10.5882\n",
      "      2        \u001b[36m0.4789\u001b[0m  10.5586\n",
      "      3        \u001b[36m0.3019\u001b[0m  10.5462\n",
      "[CV] END batch_size=32, module__dropout=0.1, optimizer__lr=2e-05; total time=  39.7s\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.6894\u001b[0m  10.6159\n",
      "      2        \u001b[36m0.5756\u001b[0m  10.5787\n",
      "      3        \u001b[36m0.3491\u001b[0m  10.6421\n",
      "[CV] END batch_size=32, module__dropout=0.1, optimizer__lr=2e-05; total time=  39.9s\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.7217\u001b[0m  10.5998\n",
      "      2        \u001b[36m0.6797\u001b[0m  10.5585\n",
      "      3        \u001b[36m0.6068\u001b[0m  10.5642\n",
      "[CV] END batch_size=32, module__dropout=0.3, optimizer__lr=1e-05; total time=  39.8s\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.7190\u001b[0m  10.6501\n",
      "      2        \u001b[36m0.6813\u001b[0m  10.5963\n",
      "      3        \u001b[36m0.6065\u001b[0m  10.5903\n",
      "[CV] END batch_size=32, module__dropout=0.3, optimizer__lr=1e-05; total time=  40.0s\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.7426\u001b[0m  10.6025\n",
      "      2        \u001b[36m0.6520\u001b[0m  10.5425\n",
      "      3        \u001b[36m0.4506\u001b[0m  10.5495\n",
      "[CV] END batch_size=32, module__dropout=0.5, optimizer__lr=2e-05; total time=  39.8s\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.7396\u001b[0m  10.6034\n",
      "      2        \u001b[36m0.6897\u001b[0m  10.5918\n",
      "      3        \u001b[36m0.5863\u001b[0m  10.5700\n",
      "[CV] END batch_size=32, module__dropout=0.5, optimizer__lr=2e-05; total time=  39.8s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6731\u001b[0m  8.5831\n",
      "      2        \u001b[36m0.5493\u001b[0m  8.5258\n",
      "      3        \u001b[36m0.3622\u001b[0m  8.6020\n",
      "[CV] END batch_size=64, module__dropout=0.1, optimizer__lr=2e-05; total time=  33.1s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7124\u001b[0m  8.6349\n",
      "      2        \u001b[36m0.6590\u001b[0m  8.5976\n",
      "      3        \u001b[36m0.6350\u001b[0m  8.5622\n",
      "[CV] END batch_size=64, module__dropout=0.1, optimizer__lr=2e-05; total time=  33.2s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6970\u001b[0m  8.5783\n",
      "      2        \u001b[36m0.6205\u001b[0m  8.5260\n",
      "      3        \u001b[36m0.4601\u001b[0m  8.4805\n",
      "[CV] END batch_size=64, module__dropout=0.2, optimizer__lr=2e-05; total time=  33.0s\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7249\u001b[0m  8.5213\n",
      "      2        \u001b[36m0.6595\u001b[0m  8.5303\n",
      "      3        \u001b[36m0.5831\u001b[0m  8.5263\n",
      "[CV] END batch_size=64, module__dropout=0.2, optimizer__lr=2e-05; total time=  32.7s\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 5\n",
      "n_resources: 4000\n",
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.6753\u001b[0m  20.7415\n",
      "      2        \u001b[36m0.5250\u001b[0m  21.0619\n",
      "      3        \u001b[36m0.2997\u001b[0m  20.9744\n",
      "[CV] END batch_size=32, module__dropout=0.3, optimizer__lr=2e-05; total time= 1.3min\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.6726\u001b[0m  21.0095\n",
      "      2        \u001b[36m0.4847\u001b[0m  20.9677\n",
      "      3        \u001b[36m0.2852\u001b[0m  20.9214\n",
      "[CV] END batch_size=32, module__dropout=0.3, optimizer__lr=2e-05; total time= 1.3min\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.7157\u001b[0m  20.9975\n",
      "      2        \u001b[36m0.6398\u001b[0m  20.9295\n",
      "      3        \u001b[36m0.5030\u001b[0m  20.9795\n",
      "[CV] END batch_size=32, module__dropout=0.5, optimizer__lr=2e-05; total time= 1.3min\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.6753\u001b[0m  20.9835\n",
      "      2        \u001b[36m0.4974\u001b[0m  20.8745\n",
      "      3        \u001b[36m0.3059\u001b[0m  20.7766\n",
      "[CV] END batch_size=32, module__dropout=0.5, optimizer__lr=2e-05; total time= 1.3min\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.6699\u001b[0m  20.7294\n",
      "      2        \u001b[36m0.5078\u001b[0m  20.7686\n",
      "      3        \u001b[36m0.3121\u001b[0m  20.7790\n",
      "[CV] END batch_size=32, module__dropout=0.4, optimizer__lr=2e-05; total time= 1.2min\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.6931\u001b[0m  20.8836\n",
      "      2        \u001b[36m0.5140\u001b[0m  20.7753\n",
      "      3        \u001b[36m0.3206\u001b[0m  20.8217\n",
      "[CV] END batch_size=32, module__dropout=0.4, optimizer__lr=2e-05; total time= 1.2min\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.6440\u001b[0m  21.0465\n",
      "      2        \u001b[36m0.4496\u001b[0m  21.0253\n",
      "      3        \u001b[36m0.2565\u001b[0m  21.1083\n",
      "[CV] END batch_size=32, module__dropout=0.1, optimizer__lr=2e-05; total time= 1.3min\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.6154\u001b[0m  21.0206\n",
      "      2        \u001b[36m0.4069\u001b[0m  21.0918\n",
      "      3        \u001b[36m0.2158\u001b[0m  21.0359\n",
      "[CV] END batch_size=32, module__dropout=0.1, optimizer__lr=2e-05; total time= 1.3min\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.6995\u001b[0m  17.0888\n",
      "      2        \u001b[36m0.5559\u001b[0m  17.0114\n",
      "      3        \u001b[36m0.3638\u001b[0m  16.9710\n",
      "[CV] END batch_size=64, module__dropout=0.2, optimizer__lr=2e-05; total time= 1.0min\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.6931\u001b[0m  17.0652\n",
      "      2        \u001b[36m0.5474\u001b[0m  16.9830\n",
      "      3        \u001b[36m0.3585\u001b[0m  16.9036\n",
      "[CV] END batch_size=64, module__dropout=0.2, optimizer__lr=2e-05; total time= 1.0min\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 3\n",
      "n_resources: 8000\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.6281\u001b[0m  41.8312\n",
      "      2        \u001b[36m0.4274\u001b[0m  41.8768\n",
      "      3        \u001b[36m0.2423\u001b[0m  41.4581\n",
      "[CV] END batch_size=32, module__dropout=0.5, optimizer__lr=2e-05; total time= 2.5min\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.6244\u001b[0m  41.5638\n",
      "      2        \u001b[36m0.4075\u001b[0m  41.4867\n",
      "      3        \u001b[36m0.2136\u001b[0m  41.5851\n",
      "[CV] END batch_size=32, module__dropout=0.5, optimizer__lr=2e-05; total time= 2.4min\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.6350\u001b[0m  34.0398\n",
      "      2        \u001b[36m0.4446\u001b[0m  33.7059\n",
      "      3        \u001b[36m0.2805\u001b[0m  33.6191\n",
      "[CV] END batch_size=64, module__dropout=0.2, optimizer__lr=2e-05; total time= 2.0min\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.6523\u001b[0m  33.8383\n",
      "      2        \u001b[36m0.4714\u001b[0m  33.8607\n",
      "      3        \u001b[36m0.3006\u001b[0m  34.0641\n",
      "[CV] END batch_size=64, module__dropout=0.2, optimizer__lr=2e-05; total time= 2.0min\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.5972\u001b[0m  41.7915\n",
      "      2        \u001b[36m0.3766\u001b[0m  41.9753\n",
      "      3        \u001b[36m0.2153\u001b[0m  41.7805\n",
      "[CV] END batch_size=32, module__dropout=0.3, optimizer__lr=2e-05; total time= 2.5min\n",
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m0.5940\u001b[0m  41.6024\n",
      "      2        \u001b[36m0.3769\u001b[0m  41.4152\n",
      "      3        \u001b[36m0.1866\u001b[0m  41.5881\n",
      "[CV] END batch_size=32, module__dropout=0.3, optimizer__lr=2e-05; total time= 2.4min\n",
      "0.7705676419104777\n",
      "{'batch_size': 32, 'module__dropout': 0.3, 'optimizer__lr': 2e-05}\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from skorch.classifier import NeuralNetClassifier, NeuralNetBinaryClassifier\n",
    "\n",
    "# Note: when defining arguments below, the syntax [object_name]__[argument]=[value]\n",
    "# defines the parameter for the given object\n",
    "# E.g. SentimentClassifier is given module__n_classes as argument\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    module=SentimentClassifier,\n",
    "    optimizer=AdamW,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    max_epochs=3,\n",
    "    verbose=1,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    train_split=None, # sklearn grid search already does this\n",
    "    device=\"cuda\",\n",
    "    # optimizer args\n",
    "    optimizer__lr=LR,\n",
    "    # module args\n",
    "    module__n_classes=2,\n",
    "    module__dropout=0.3,\n",
    ")\n",
    "\n",
    "# here we define parameters to perform Grid Search on\n",
    "params = {\n",
    "    'module__dropout': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'optimizer__lr': [2e-5, 1e-5],\n",
    "    'batch_size': [32, 64]\n",
    "}\n",
    "\n",
    "# refit: Refit an estimator using the best found parameters on the whole dataset\n",
    "# cv: Controls number of folds with StratifiedKFold\n",
    "# scoring: for classification we use accuracy, or balanced_accuracy if dealing with imbalanced dataset\n",
    "\n",
    "# to get an estimate of cell runtime for GridSearchCV, calculate\n",
    "# len(params_1) x ... x len(params_n) x cv x max_epochs x one_epoch_runtime\n",
    "# with HalvingGridSearchCV its not so linear because epoch runtimes change\n",
    "\n",
    "gs = HalvingGridSearchCV(\n",
    "    net, \n",
    "    params, \n",
    "    refit=False, # no need refit using best parameters for now\n",
    "    scoring='accuracy', # switch to balanced_accuracy with uneven datasets\n",
    "    min_resources=1000, # resources to use in iter=0 of halving grid search\n",
    "    factor=2,\n",
    "    max_resources=8000, # max resources to use in iter=n of halving grid search\n",
    "    cv=2,\n",
    "    verbose=2\n",
    "    ) \n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "best_score, best_params = gs.best_score_, gs.best_params_\n",
    "\n",
    "print(best_score)\n",
    "print(best_params)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "twitter-bert-gridsearch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
