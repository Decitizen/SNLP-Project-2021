{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text data pre-processing\n",
    "## **SNLP team project**\n",
    "In this notebook we do the second phase of preprocessing by selecting the relevant data fields from the json object and discarding unrelevant ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_texts(data):\n",
    "    \"\"\"Extract tweet texts from pandas dataframe.\"\"\"\n",
    "    texts = []\n",
    "    for indx, row in data.iterrows():\n",
    "        status_obj = row['retweeted_status'] if row['is_retweet'] else row \n",
    "        if status_obj['truncated']:\n",
    "            texts.append(status_obj['extended_tweet']['full_text'])\n",
    "        else:\n",
    "            texts.append(status_obj['text'])\n",
    "    \n",
    "    return texts\n",
    "\n",
    "def pre_process_dataframe(data):    \n",
    "    # Restructure tweets\n",
    "    data['is_retweet']  = [1 if b else 0 for b in data['retweeted_status'].notnull()] \n",
    "    data['content']     = extract_texts(data)\n",
    "    data['id_str']      = data['id_str'].astype(str)\n",
    "    data['user_id_str'] = [row['id_str'] for row in data['user']]\n",
    "    \n",
    "    parent_tweet_ids = []\n",
    "    parent_user_ids  = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        if row['is_retweet']:\n",
    "            tweet = row['retweeted_status']\n",
    "            parent_tweet_ids.append(tweet['id_str'])\n",
    "            parent_user_ids.append(tweet['user']['id_str'])\n",
    "        else:\n",
    "            parent_tweet_ids.append(None)\n",
    "            parent_user_ids.append(None)\n",
    "\n",
    "    data['parent_tweet_id_str'] = parent_tweet_ids\n",
    "    data['parent_user_id_str']  = parent_user_ids\n",
    "\n",
    "    selection = ['content',             # Textual content\n",
    "                 'created_at',          # Timestamp\n",
    "                 'id_str',              # Id of the tweet for linking back later \n",
    "                 'user_id_str',         # Identify users \n",
    "                 'parent_tweet_id_str', # If necessary link with retweet parent\n",
    "                 'parent_user_id_str',  # If necessary link with retweet parent's author\n",
    "                 'coordinates']         # Geocoordinates\n",
    "                                      \n",
    "    data = data[selection]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load id mapping \n",
    "See network preprocessing notebook for the id mapping routines. Here we will use the mapping and update it where necessary. We will also add mapping for tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mappings(map_path):\n",
    "    \"\"\"\n",
    "    Loads user and tweet pseudonymization mappings from given folder path.\n",
    "    \n",
    "    Returns two lists containing:\n",
    "    mappings (list): list consisting of two dict objects in the order [user_map, tweet_map]\n",
    "          Ss (list): list consisting of two set objects each containing the set of used name space \n",
    "    \"\"\"\n",
    "    u_map_path = map_path + 'user_mapping.csv'\n",
    "    t_map_path = map_path + 'tweet_mapping.csv'\n",
    "    \n",
    "    mappings, Ss = []\n",
    "    for path, ttype in zip([u_map_path, t_map_path], ['user','tweet']):\n",
    "        if path.exists(path):\n",
    "            ttype_map = pd.read_csv(path, sep=',')\n",
    "            \n",
    "            # Keep set of used ids in the possible id space for \n",
    "            # avoiding collision / fast checks\n",
    "            ttype_S = set(ttype_map['new_id'].values)\n",
    "            \n",
    "            ttype_map['orig_id'] = ttype_map.orig_id.astype(str)\n",
    "            ttype_map = dict(ttype_map[['orig_id','new_id']].values)\n",
    "        else:\n",
    "            ttype_S = set\n",
    "            ttype_map = {}\n",
    "            print('Warning! No existing {} id mapping was found, initialized new mapping.'.format(ttype))\n",
    "    \n",
    "    return mappings, Ss\n",
    "\n",
    "def save_mappings(user_map, tweet_map, map_path):\n",
    "    u_map_path = map_path + 'user_mapping.csv'\n",
    "    t_map_path = map_path + 'tweet_mapping.csv'\n",
    "    \n",
    "    for path, ttype_map in zip([u_map_path, t_map_path], [user_map, tweet_map]):\n",
    "        with open(path, 'w') as file:\n",
    "            file.write('new_id,orig_id')\n",
    "            for orig_id, new_id in ttype_map.items():\n",
    "                file.write('\\n{},{}'.format(new_id,orig_id))\n",
    "    \n",
    "def map_tweet(t_id):\n",
    "    assert isinstance(t_id, str), 'Not a string, check input type'\n",
    "    \n",
    "    if t_id in tweet_map:\n",
    "        return tweet_map[t_id]\n",
    "    else:\n",
    "        while True:\n",
    "            new_id = np.random.randint(0,2e8)\n",
    "            if new_id not in T:\n",
    "                T.add(new_id)\n",
    "                break\n",
    "        tweet_map[t_id] = new_id\n",
    "        return new_id\n",
    "\n",
    "def map_user(u_id):\n",
    "    assert isinstance(u_id, str), 'Not a string, check input type'\n",
    "    \n",
    "    if u_id in user_map:\n",
    "        return user_map[u_id]\n",
    "    else:\n",
    "        while True:\n",
    "            new_id = np.random.randint(0,2e8)\n",
    "            if new_id not in S:\n",
    "                S.add(new_id)\n",
    "                break\n",
    "        user_map[u_id] = new_id\n",
    "        return new_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_path = '../../tweet_data/id_mapping/'\n",
    "mappings, Ss = load_mappings(map_path)\n",
    "\n",
    "S, T = Ss[0], Ss[1]\n",
    "user_map, tweet_map = mappings[0], mappings[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "We will be going through all of the keyword streaming data files and preprocessing them one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = !find ../../tweet_data/filtered/tweets_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "csize = 200000\n",
    "data_file = None\n",
    "savepath = '../../tweet_data/preprocessed/text/tweets_climate_en_text_{}.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_count = 0\n",
    "iter_times = []\n",
    "n_extracted_ls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing file: ../../tweet_data/filtered/tweets_climate_en_20200812.json\n",
      "* Number of tweets processed: 200000 per file\n",
      "* Relevant tweets identified: \n",
      "\t\t\t- 200000 iteration / 200000 overall\n",
      "\t\t\t- On avg: 200000.00 iteration (+/-) 0\n",
      "\t\t\t-         100.00 %         (+/-) 0.00 %\n",
      "* Time for iteration 1: 68 s\n",
      "* Avg time for iteration: 68.2 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Number of tweets processed: 400000 per file\n",
      "* Relevant tweets identified: \n",
      "\t\t\t- 200000 iteration / 400000 overall\n",
      "\t\t\t- On avg: 200000.00 iteration (+/-) 0\n",
      "\t\t\t-         100.00 %         (+/-) 0.00 %\n",
      "* Time for iteration 2: 67 s\n",
      "* Avg time for iteration: 67.7 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Number of tweets processed: 600000 per file\n",
      "* Relevant tweets identified: \n",
      "\t\t\t- 200000 iteration / 600000 overall\n",
      "\t\t\t- On avg: 200000.00 iteration (+/-) 0\n",
      "\t\t\t-         100.00 %         (+/-) 0.00 %\n",
      "* Time for iteration 3: 66 s\n",
      "* Avg time for iteration: 67.1 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Number of tweets processed: 800000 per file\n",
      "* Relevant tweets identified: \n",
      "\t\t\t- 200000 iteration / 800000 overall\n",
      "\t\t\t- On avg: 200000.00 iteration (+/-) 0\n",
      "\t\t\t-         100.00 %         (+/-) 0.00 %\n",
      "* Time for iteration 4: 66 s\n",
      "* Avg time for iteration: 66.9 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Number of tweets processed: 1000000 per file\n",
      "* Relevant tweets identified: \n",
      "\t\t\t- 200000 iteration / 1000000 overall\n",
      "\t\t\t- On avg: 200000.00 iteration (+/-) 0\n",
      "\t\t\t-         100.00 %         (+/-) 0.00 %\n",
      "* Time for iteration 5: 67 s\n",
      "* Avg time for iteration: 66.9 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Number of tweets processed: 1200000 per file\n",
      "* Relevant tweets identified: \n",
      "\t\t\t- 200000 iteration / 1200000 overall\n",
      "\t\t\t- On avg: 200000.00 iteration (+/-) 0\n",
      "\t\t\t-         100.00 %         (+/-) 0.00 %\n",
      "* Time for iteration 6: 72 s\n",
      "* Avg time for iteration: 67.8 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Number of tweets processed: 1400000 per file\n",
      "* Relevant tweets identified: \n",
      "\t\t\t- 200000 iteration / 1400000 overall\n",
      "\t\t\t- On avg: 200000.00 iteration (+/-) 0\n",
      "\t\t\t-         100.00 %         (+/-) 0.00 %\n",
      "* Time for iteration 7: 69 s\n",
      "* Avg time for iteration: 68.0 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Number of tweets processed: 1600000 per file\n",
      "* Relevant tweets identified: \n",
      "\t\t\t- 200000 iteration / 1600000 overall\n",
      "\t\t\t- On avg: 200000.00 iteration (+/-) 0\n",
      "\t\t\t-         100.00 %         (+/-) 0.00 %\n",
      "* Time for iteration 8: 74 s\n",
      "* Avg time for iteration: 68.7 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Number of tweets processed: 1800000 per file\n",
      "* Relevant tweets identified: \n",
      "\t\t\t- 14860 iteration / 1614860 overall\n",
      "\t\t\t- On avg: 179428.89 iteration (+/-) 58184\n",
      "\t\t\t-         89.71 %         (+/-) 29.09 %\n",
      "* Time for iteration 9: 4 s\n",
      "* Avg time for iteration: 61.5 s\n",
      "==================================================================================================== \n",
      "\n",
      "==================================================\n",
      "Time for iteration: 9.0 minutes, 32.99 seconds\n",
      "Estimated time left: 0.0 hours, 0.0 minutes, 0.00 seconds\n",
      "Ready with file: ../../tweet_data/filtered/tweets_climate_en_20200812.json\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "times = []\n",
    "\n",
    "for i,fpath in enumerate(filepaths):\n",
    "    print('Preprocessing file: {}'.format(fpath))\n",
    "    stime = time.time()\n",
    "    date = re.findall('[0-9]+', fpath)[0]\n",
    "    data_iter = pd.read_json(fpath,  orient = \"records\", \n",
    "                        dtype = False, lines = True, \n",
    "                        encoding = \"utf-8\", chunksize = csize)\n",
    "\n",
    "    data = iter(data_iter)\n",
    "    \n",
    "    j = 0\n",
    "    while True:\n",
    "        s_time_iter = time.time()\n",
    "        try:\n",
    "            df_chunk = next(data)\n",
    "            # Preprocess the dataframe\n",
    "            df_chunk = pre_process_dataframe(df_chunk)\n",
    "\n",
    "            if data_file is None:\n",
    "                data_file = df_chunk.copy()\n",
    "            else:\n",
    "                data_file = pd.concat([data_file, df_chunk])\n",
    "\n",
    "            n_extracted = df_chunk.shape[0]\n",
    "            n_extracted_ls.append(n_extracted)\n",
    "            global_count += df_chunk.shape[0]\n",
    "\n",
    "            #Print iteration stats\n",
    "            iter_time = time.time()-s_time_iter\n",
    "            iter_times.append(iter_time)\n",
    "\n",
    "            print('* Number of tweets processed: {} per file'.format((j+1)*csize))\n",
    "            print('* Relevant tweets identified: \\n\\t\\t\\t- {} iteration / {} overall'.format(n_extracted, global_count)) \n",
    "            print('\\t\\t\\t- On avg: {:.2f} iteration (+/-) {:.0f}'.format(np.mean(n_extracted_ls), np.std(n_extracted_ls)))\n",
    "            print('\\t\\t\\t-         {:.2f} %         (+/-) {:.2f} %'.format(100*np.mean(n_extracted_ls) / csize, 100*np.std(n_extracted_ls) / csize))\n",
    "            print('* Time for iteration {}: {:.0f} s'.format(j+1,iter_time))\n",
    "            print('* Avg time for iteration: {:.1f} s'.format(np.mean(iter_times)))\n",
    "            print(50*'==','\\n')\n",
    "\n",
    "        except StopIteration:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print('In file: {} at {}th iteration, exception occurred: {}'.format(fpath,j,e))\n",
    "            j += 1\n",
    "            continue\n",
    "        \n",
    "        j += 1\n",
    "                \n",
    "        # Pseudonymize by mapping\n",
    "        data_file['id_str'] = data_file['id_str'].apply(lambda x: map_tweet(str(x)))\n",
    "        data_file['parent_tweet_id_str'] = data_file['parent_tweet_id_str'].apply(lambda x: map_tweet(str(x)))\n",
    "        data_file['user_id_str'] = data_file['user_id_str'].apply(lambda x: map_user(str(x)))\n",
    "        data_file['parent_user_id_str'] = data_file['parent_user_id_str'].apply(lambda x: map_user(str(x)))\n",
    "        \n",
    "        with open(savepath.format(date), 'a') as file:\n",
    "            data_file.to_json(file, orient = \"records\", lines = True)\n",
    "        \n",
    "        data_file = None\n",
    "    \n",
    "    offset = time.time() - stime\n",
    "    times.append(offset)\n",
    "    m_time = np.mean(times)\n",
    "    \n",
    "    k = len(filepaths) - (i+1)\n",
    "    t_left = m_time * k \n",
    "    h_left = t_left // 3600\n",
    "    m_left = (t_left % 3600) // 60\n",
    "    s_left = (t_left % 60) \n",
    "    print(50*'=')\n",
    "    print('Time for iteration: {0} minutes, {1:.2f} seconds'.format(offset // 60, offset % 60 ))\n",
    "    print('Estimated time left: {0} hours, {1} minutes, {2:.2f} seconds'.format(h_left, m_left, s_left))\n",
    "    print('Ready with file: {}'.format(fpath.replace('../../data_collection/content-based/', '')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save node and tweet id maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_mappings(user_map, tweet_map, map_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcn",
   "language": "python",
   "name": "gcn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
