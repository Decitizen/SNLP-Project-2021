{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select tweets | Keyword streaming \n",
    "## **SNLP team project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, the original raw dataset will be preprocessed via simple filtering to ensure quality of the data.\n",
    "\n",
    "**Notebook implements following functionality:**\n",
    "* Drop redundant tweet types \n",
    "* Drop corrupted tweets\n",
    "* Filter based on language\n",
    "* Write tweets as json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(data, LANG):\n",
    "    bool_arr = []\n",
    "    \n",
    "    for text in data['text']:\n",
    "        not_lang = not is_language(LANG, text, bool_arr)\n",
    "        bool_arr.append(not_lang)\n",
    "        \n",
    "    data.drop(data[bool_arr].index, inplace=True)\n",
    "\n",
    "def is_language(lang, text, bool_arr):\n",
    "    try:\n",
    "        if langdetect.detect(text) == lang:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def drop_non_statuses(data):\n",
    "    notice_types = ['delete','event','direct_message',\n",
    "                    'friends','limit','disconnect',\n",
    "                    'warning','scrub_geo','status_withheld','user_withheld']\n",
    "    \n",
    "    for notice_type in notice_types:\n",
    "        if notice_type in data.columns:\n",
    "            data.drop(data[data[notice_type].notnull()].index, inplace=True)    \n",
    "\n",
    "def select_tweets(data):\n",
    "    # 1. Drop queried msg's other than statuses\n",
    "    drop_non_statuses(data)\n",
    "    \n",
    "    # 2. Select tweets based on language and use langdetect if feasible \n",
    "    LANG = 'en'\n",
    "    DETECT_LANG = False\n",
    "    \n",
    "    cond1_arr = data['lang'] == LANG\n",
    "    if DETECT_LANG:\n",
    "        data_non_fi = data.copy()\n",
    "        \n",
    "        # Keep lang == fi\n",
    "        data.drop(data[cond1_arr == False].index, inplace=True)\n",
    "        \n",
    "        # For set of 'lang' != 'fi' tweets, detect using langdetect\n",
    "        data_non_fi.drop(data_non_fi[cond1_arr].index, inplace=True)\n",
    "        orig_shape = data_non_fi.shape\n",
    "        detect_language(data_non_fi,LANG)\n",
    "\n",
    "        # Combine sets\n",
    "        data = pd.concat([data,data_non_fi])\n",
    "        print('\\nLang detection completed:')\n",
    "        print('\\t* {} additional tweets identified in batch of {}'.format(data_non_fi.shape[0], csize))\n",
    "        print('\\t* {:.2f} % of the non-fi set\\n'.format(data_non_fi.shape[0] / orig_shape[0] * 100))\n",
    "        \n",
    "    else:\n",
    "        data.drop(data[cond1_arr == False].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawpath = '../../tweet_data/raw/tweets_climate_en_20200812.txt'\n",
    "savepath = '../../tweet_data/filtered/tweets_climate_en_{}.json'\n",
    "\n",
    "csize = 200000\n",
    "data_file = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_count = 0\n",
    "iter_times = []\n",
    "n_extracted_ls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing file: ../../tweet_data/raw/tweets_climate_en_20200812.txt\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "times = []\n",
    "\n",
    "for i,fpath in enumerate([rawpath]):\n",
    "    print('Preprocessing file: {}'.format(fpath))\n",
    "    stime = time.time()\n",
    "    date = re.findall('[0-9]+', fpath)[0]\n",
    "    data_iter = pd.read_json(fpath,  orient = \"records\", \n",
    "                        dtype = False, lines = True, \n",
    "                        encoding = \"utf-8\", chunksize = csize)\n",
    "\n",
    "    data = iter(data_iter)\n",
    "    nodes = {}\n",
    "    edges = {}\n",
    "    \n",
    "    j = 0\n",
    "    while True:\n",
    "        s_time_iter = time.time()\n",
    "        try:\n",
    "            df_chunk = next(data)\n",
    "        \n",
    "            # Preprocess the dataframe\n",
    "            select_tweets(df_chunk)\n",
    "            \n",
    "            if data_file is None:\n",
    "                data_file = df_chunk.copy()\n",
    "            else:\n",
    "                data_file = pd.concat([data_file, df_chunk])\n",
    "        \n",
    "            n_extracted = df_chunk.shape[0]\n",
    "            n_extracted_ls.append(n_extracted)\n",
    "            global_count += df_chunk.shape[0]\n",
    "            \n",
    "            #Print iteration stats\n",
    "            iter_time = time.time()-s_time_iter\n",
    "            iter_times.append(iter_time)\n",
    "            \n",
    "            print('* Number of tweets processed: {} per file'.format((j+1)*csize))\n",
    "            print('* Relevant tweets identified: \\n\\t\\t\\t- {} iteration / {} overall'.format(n_extracted, global_count)) \n",
    "            print('\\t\\t\\t- On avg: {:.2f} iteration (+/-) {:.0f}'.format(np.mean(n_extracted_ls), np.std(n_extracted_ls)))\n",
    "            print('\\t\\t\\t-         {:.2f} %         (+/-) {:.2f} %'.format(100*np.mean(n_extracted_ls) / csize, 100*np.std(n_extracted_ls) / csize))\n",
    "            print('* Time for iteration {}: {:.0f} s'.format(j+1,iter_time))\n",
    "            print('* Avg time for iteration: {:.1f} s'.format(np.mean(iter_times)))\n",
    "            print(50*'==','\\n')\n",
    "            \n",
    "        except StopIteration:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print('In file: {} at {}th iteration, exception occurred: {}'.format(fpath,j,e))\n",
    "            j += 1\n",
    "            continue\n",
    "        \n",
    "        j += 1\n",
    "    \n",
    "        with open(savepath.format(date), 'a') as file:\n",
    "            data_file.to_json(file, orient = \"records\", lines = True)\n",
    "        \n",
    "        data_file = None\n",
    "    \n",
    "    offset = time.time() - stime\n",
    "    times.append(offset)\n",
    "    m_time = np.mean(times)\n",
    "    \n",
    "    k = len(filepaths) - (i+1)\n",
    "    t_left = m_time * k \n",
    "    h_left = t_left // 3600\n",
    "    m_left = (t_left % 3600) // 60\n",
    "    s_left = (t_left % 60) \n",
    "    print(50*'=')\n",
    "    print('Time for iteration: {0} minutes, {1:.2f} seconds'.format(offset // 60, offset % 60 ))\n",
    "    print('Estimated time left: {0} hours, {1} minutes, {2:.2f} seconds'.format(h_left, m_left, s_left))\n",
    "    print('Ready with file: {}'.format(fpath.replace('../../data_collection/content-based/', '')))\n",
    "    print('\\t* # of edges: {0}\\n\\t* # of nodes: {1}\\n'.format(len(edges), len(nodes.keys())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcn",
   "language": "python",
   "name": "gcn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
