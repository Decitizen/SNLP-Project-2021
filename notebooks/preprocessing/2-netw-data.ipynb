{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network data pre-processing\n",
    "## **SNLP team project**\n",
    "In this notebook we construct the network representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_dataframe(data):    \n",
    "    # Restructure tweets\n",
    "    data['id_str']                  = data['id_str'].astype(str)\n",
    "    data['in_reply_to_user_id_str'] = data['in_reply_to_user_id_str'].astype(str)\n",
    "    data['user_id_str']             = [row['id_str'] for row in data['user']]\n",
    "    data['is_retweet']              = [1 if b else 0 for b in data['retweeted_status'].notnull()]\n",
    "\n",
    "    parent_tweet_ids = []\n",
    "    parent_user_ids  = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        if row['is_retweet']:\n",
    "            tweet = row['retweeted_status']\n",
    "            parent_tweet_ids.append(tweet['id_str'])\n",
    "            parent_user_ids.append(tweet['user']['id_str'])\n",
    "        else:\n",
    "            parent_tweet_ids.append(None)\n",
    "            parent_user_ids.append(None)\n",
    "\n",
    "    data['parent_tweet_id_str'] = parent_tweet_ids\n",
    "    data['parent_user_id_str']  = parent_user_ids\n",
    "    data['mentions'] = data['entities'].apply(lambda x: extract_mentions(x))\n",
    "\n",
    "    selection = ['id_str',                                      # Id of the tweet for collecting replies / retweets\n",
    "                 'user_id_str',                                 # Identify / define nodes in the network \n",
    "                 'mentions',                                    # Define interaction-edges for mentions\n",
    "                 'parent_tweet_id_str',\n",
    "                 'parent_user_id_str',                          # Retweets\n",
    "                 'in_reply_to_user_id_str']                     # Define interaction-edges for replies\n",
    "\n",
    "    data = data[selection]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mentions(x):\n",
    "    return [str(n.replace(\"id': \",\"\")) for n in re.findall(\"id': [0-9]*\", str(x))]\n",
    "\n",
    "def init_edge(src,trg,edges):\n",
    "    if (str(src),str(trg)) not in edges: edges[(src,trg)] = Counter({'orig_mentions':0, 'retweets':0, 'replies':0})            \n",
    "\n",
    "def extract_edges(data: pd.DataFrame, edges: dict) -> dict:\n",
    "    errors = []\n",
    "    \n",
    "    for i,row in data.iterrows():\n",
    "        src      = row['user_id_str']\n",
    "        par_id   = row['parent_user_id_str']\n",
    "        reply_id = row['in_reply_to_user_id_str'] \n",
    "        mentions = list(row['mentions'])\n",
    "        \n",
    "        # Note that mentions are superset of the union of retweets and replies, thus we \n",
    "        # first remove both of them, and then the authentic mentions are left.\n",
    "        \n",
    "        # Case 1. replies\n",
    "        if reply_id in mentions:\n",
    "            init_edge(src,reply_id,edges)\n",
    "            edges[(src,reply_id)]['replies'] += 1\n",
    "            mentions.remove(reply_id)\n",
    "        \n",
    "        # Case 2. retweets\n",
    "        if par_id is not None:\n",
    "            init_edge(src,par_id,edges)\n",
    "            edges[(src,par_id)]['retweets'] += 1\n",
    "            try:\n",
    "                mentions.remove(par_id)\n",
    "            except Exception as ex:\n",
    "                errors.append((ex,row['id_str'],src,par_id))\n",
    "                \n",
    "        # Case 3. mentions\n",
    "        for trg in mentions:\n",
    "            init_edge(src,trg,edges)\n",
    "            edges[(src,trg)]['orig_mentions'] += 1            \n",
    "    \n",
    "        with open('pre_processing_errors.log', 'w') as f:\n",
    "            for er in errors:\n",
    "                f.write('\\n===\\nEX: {0}\\nTWID: {1}\\nSRC_ID: {2}\\nPAR_ID: {3}'.format(\n",
    "                    er[0],er[1],er[2],er[3]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "We will be going through all of the keyword streaming data files and preprocessing them one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = !find ../../tweet_data/filtered/tweets_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../tweet_data/filtered/tweets_climate_en_20200812.json']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "csize = 50000\n",
    "debug = True\n",
    "edge_path = '../../tweet_data/preprocessed/network/edges_{0}.json'\n",
    "user_map_path = '../../tweet_data/id_mapping/user_mapping.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_count = 0\n",
    "iter_times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing file: ../../tweet_data/filtered/tweets_climate_en_20200812.json\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 50000\n",
      "* Time for iteration 1: 14 s\n",
      "* Avg time for iteration: 13.8 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 1: 14 s\n",
      "* Avg time for iteration: 13.8 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 100000\n",
      "* Time for iteration 2: 13 s\n",
      "* Avg time for iteration: 13.4 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 2: 13 s\n",
      "* Avg time for iteration: 13.4 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 150000\n",
      "* Time for iteration 3: 12 s\n",
      "* Avg time for iteration: 13.1 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 3: 12 s\n",
      "* Avg time for iteration: 13.1 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 200000\n",
      "* Time for iteration 4: 12 s\n",
      "* Avg time for iteration: 12.7 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 4: 12 s\n",
      "* Avg time for iteration: 12.7 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 250000\n",
      "* Time for iteration 5: 12 s\n",
      "* Avg time for iteration: 12.7 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 5: 12 s\n",
      "* Avg time for iteration: 12.7 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 300000\n",
      "* Time for iteration 6: 14 s\n",
      "* Avg time for iteration: 12.8 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 6: 14 s\n",
      "* Avg time for iteration: 12.8 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 350000\n",
      "* Time for iteration 7: 12 s\n",
      "* Avg time for iteration: 12.7 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 7: 12 s\n",
      "* Avg time for iteration: 12.7 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 400000\n",
      "* Time for iteration 8: 13 s\n",
      "* Avg time for iteration: 12.7 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 8: 13 s\n",
      "* Avg time for iteration: 12.7 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 450000\n",
      "* Time for iteration 9: 12 s\n",
      "* Avg time for iteration: 12.6 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 9: 12 s\n",
      "* Avg time for iteration: 12.6 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 500000\n",
      "* Time for iteration 10: 12 s\n",
      "* Avg time for iteration: 12.5 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 10: 12 s\n",
      "* Avg time for iteration: 12.5 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 550000\n",
      "* Time for iteration 11: 13 s\n",
      "* Avg time for iteration: 12.5 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 11: 13 s\n",
      "* Avg time for iteration: 12.5 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 600000\n",
      "* Time for iteration 12: 12 s\n",
      "* Avg time for iteration: 12.5 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 12: 12 s\n",
      "* Avg time for iteration: 12.5 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 650000\n",
      "* Time for iteration 13: 13 s\n",
      "* Avg time for iteration: 12.6 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 13: 13 s\n",
      "* Avg time for iteration: 12.6 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 700000\n",
      "* Time for iteration 14: 12 s\n",
      "* Avg time for iteration: 12.5 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 14: 12 s\n",
      "* Avg time for iteration: 12.5 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 750000\n",
      "* Time for iteration 15: 12 s\n",
      "* Avg time for iteration: 12.5 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 15: 12 s\n",
      "* Avg time for iteration: 12.5 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 800000\n",
      "* Time for iteration 16: 13 s\n",
      "* Avg time for iteration: 12.5 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 16: 13 s\n",
      "* Avg time for iteration: 12.5 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 850000\n",
      "* Time for iteration 17: 12 s\n",
      "* Avg time for iteration: 12.5 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 17: 12 s\n",
      "* Avg time for iteration: 12.5 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 900000\n",
      "* Time for iteration 18: 12 s\n",
      "* Avg time for iteration: 12.4 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 18: 12 s\n",
      "* Avg time for iteration: 12.4 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 950000\n",
      "* Time for iteration 19: 13 s\n",
      "* Avg time for iteration: 12.5 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 19: 13 s\n",
      "* Avg time for iteration: 12.5 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 1000000\n",
      "* Time for iteration 20: 12 s\n",
      "* Avg time for iteration: 12.4 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 20: 12 s\n",
      "* Avg time for iteration: 12.4 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 1050000\n",
      "* Time for iteration 21: 13 s\n",
      "* Avg time for iteration: 12.5 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 21: 13 s\n",
      "* Avg time for iteration: 12.5 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 1100000\n",
      "* Time for iteration 22: 16 s\n",
      "* Avg time for iteration: 12.6 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 22: 16 s\n",
      "* Avg time for iteration: 12.6 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 1150000\n",
      "* Time for iteration 23: 13 s\n",
      "* Avg time for iteration: 12.6 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 23: 13 s\n",
      "* Avg time for iteration: 12.6 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 1200000\n",
      "* Time for iteration 24: 13 s\n",
      "* Avg time for iteration: 12.6 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 24: 13 s\n",
      "* Avg time for iteration: 12.6 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 1250000\n",
      "* Time for iteration 25: 13 s\n",
      "* Avg time for iteration: 12.7 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 25: 13 s\n",
      "* Avg time for iteration: 12.7 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 1300000\n",
      "* Time for iteration 26: 13 s\n",
      "* Avg time for iteration: 12.7 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 26: 13 s\n",
      "* Avg time for iteration: 12.7 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 1350000\n",
      "* Time for iteration 27: 13 s\n",
      "* Avg time for iteration: 12.7 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 27: 13 s\n",
      "* Avg time for iteration: 12.7 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 1400000\n",
      "* Time for iteration 28: 14 s\n",
      "* Avg time for iteration: 12.7 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 28: 14 s\n",
      "* Avg time for iteration: 12.7 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 1450000\n",
      "* Time for iteration 29: 13 s\n",
      "* Avg time for iteration: 12.8 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 29: 13 s\n",
      "* Avg time for iteration: 12.8 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 1500000\n",
      "* Time for iteration 30: 13 s\n",
      "* Avg time for iteration: 12.8 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 30: 13 s\n",
      "* Avg time for iteration: 12.8 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 1550000\n",
      "* Time for iteration 31: 14 s\n",
      "* Avg time for iteration: 12.8 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 31: 14 s\n",
      "* Avg time for iteration: 12.8 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 1600000\n",
      "* Time for iteration 32: 14 s\n",
      "* Avg time for iteration: 12.9 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 32: 14 s\n",
      "* Avg time for iteration: 12.9 s\n",
      "==================================================================================================== \n",
      "* Number of tweets processed: 1650000\n",
      "* Time for iteration 33: 3 s\n",
      "* Avg time for iteration: 12.6 s\n",
      "==================================================================================================== \n",
      "\n",
      "* Time for iteration 33: 3 s\n",
      "* Avg time for iteration: 12.6 s\n",
      "==================================================\n",
      "Time for iteration: 9.0 minutes, 52.86 seconds\n",
      "Estimated time left: 0.0 hours, 0.0 minutes, 0.00 seconds\n",
      "Ready with file: ../../tweet_data/filtered/tweets_climate_en_20200812.json\n",
      "\t* # of edges: 1653004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "times = []\n",
    "\n",
    "for i,fpath in enumerate(filepaths):\n",
    "    print('Preprocessing file: {}'.format(fpath))\n",
    "    stime = time.time()\n",
    "    date = re.findall('[0-9]+', fpath)[0]\n",
    "    data_iter = pd.read_json(fpath,  orient = \"records\", \n",
    "                        dtype = False, lines = True, \n",
    "                        encoding = \"utf-8\", chunksize = csize)\n",
    "    data = iter(data_iter)\n",
    "    edges = {}\n",
    "    \n",
    "    j = 0\n",
    "    while True:\n",
    "        s_time_iter = time.time()\n",
    "        try:\n",
    "            df_chunk = next(data)\n",
    "\n",
    "            # Preprocess the dataframe\n",
    "            df_chunk = pre_process_dataframe(df_chunk)\n",
    "            global_count += df_chunk.shape[0]\n",
    "\n",
    "            #Print iteration stats\n",
    "            iter_time = time.time()-s_time_iter\n",
    "            iter_times.append(iter_time)\n",
    "\n",
    "            print(50*'==','\\n* Number of tweets processed: {}'.format((j+1)*csize))\n",
    "            print('* Time for iteration {}: {:.0f} s'.format(j+1,iter_time))\n",
    "            print('* Avg time for iteration: {:.1f} s'.format(np.mean(iter_times)))\n",
    "            print(50*'==','\\n')\n",
    "\n",
    "            print('* Time for iteration {}: {:.0f} s'.format(j+1,iter_time))\n",
    "            print('* Avg time for iteration: {:.1f} s'.format(np.mean(iter_times)))\n",
    "\n",
    "            extract_edges(df_chunk, edges)\n",
    "\n",
    "        except StopIteration:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print('In file: {} at {}th iteration, exception occurred: {}'.format(fpath,j,e))\n",
    "            j += 1\n",
    "            continue\n",
    "        \n",
    "        j += 1\n",
    "\n",
    "    offset = time.time() - stime\n",
    "    times.append(offset)\n",
    "    m_time = np.mean(times)\n",
    "    \n",
    "    k = len(filepaths) - (i+1)\n",
    "    t_left = m_time * k \n",
    "    h_left = t_left // 3600\n",
    "    m_left = (t_left % 3600) // 60\n",
    "    s_left = (t_left % 60) \n",
    "    print(50*'=')\n",
    "    print('Time for iteration: {0} minutes, {1:.2f} seconds'.format(offset // 60, offset % 60 ))\n",
    "    print('Estimated time left: {0} hours, {1} minutes, {2:.2f} seconds'.format(h_left, m_left, s_left))\n",
    "    print('Ready with file: {}'.format(fpath.replace('../../data_collection/content-based/', '')))\n",
    "    print('\\t* # of edges: {0}\\n'.format(len(edges)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pseudonymization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a new randomized user mapping and save it (only locally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_map = {}\n",
    "S = set()\n",
    "\n",
    "for j,k in edges.items():\n",
    "    for e in [j[0],j[1]]:\n",
    "        if e not in user_map:\n",
    "            while True:\n",
    "                new_id = np.random.randint(0,2e8)\n",
    "                if new_id not in S:\n",
    "                    S.add(new_id)\n",
    "                    break\n",
    "            user_map[e] = new_id\n",
    "\n",
    "with open(user_map_path, 'w') as file:\n",
    "    file.write('new_id,orig_id')\n",
    "    for orig_id, new_id in user_map.items():\n",
    "        file.write('\\n{},{}'.format(new_id,orig_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save edge data with pseudonymized ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist = [(user_map[j[0]],user_map[j[1]],dict(k)) for j,k in edges.items()]\n",
    "with open(edge_path.format(date), 'w') as file:\n",
    "    json.dump(edgelist, file, allow_nan=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcn",
   "language": "python",
   "name": "gcn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
